{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Landscape Visualization\n",
    "\n",
    "Visualize how reward changes with gripper position, grasping state, and lift height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters - modify these to experiment\n",
    "REACHING_WEIGHT = 0.3\n",
    "REACHING_COEFF = 3.0\n",
    "GRASP_REWARD = 0.5\n",
    "LIFT_REWARD = 1.0\n",
    "SUCCESS_REWARD = 200.0\n",
    "TIME_PENALTY = 0.5\n",
    "SUCCESS_HEIGHT = 0.04  # 4cm above table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(dist, is_grasping, height_above_table, \n",
    "                   reaching_weight=REACHING_WEIGHT,\n",
    "                   reaching_coeff=REACHING_COEFF,\n",
    "                   grasp_reward=GRASP_REWARD,\n",
    "                   lift_reward=LIFT_REWARD,\n",
    "                   success_reward=SUCCESS_REWARD,\n",
    "                   time_penalty=TIME_PENALTY):\n",
    "    \"\"\"Compute reward for a single step.\"\"\"\n",
    "    reward = 0.0\n",
    "    \n",
    "    # Reaching reward\n",
    "    dist_reward = (1 - np.tanh(reaching_coeff * dist)) * reaching_weight\n",
    "    reward += dist_reward\n",
    "    \n",
    "    # Grasp and lift rewards (only if grasping)\n",
    "    if is_grasping:\n",
    "        reward += grasp_reward\n",
    "        reward += min(height_above_table, 1.0) * lift_reward\n",
    "    \n",
    "    # Success bonus\n",
    "    if is_grasping and height_above_table >= SUCCESS_HEIGHT:\n",
    "        reward += success_reward\n",
    "    \n",
    "    # Time penalty\n",
    "    reward -= time_penalty\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reaching Reward vs Distance (comparing coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.linspace(0, 1.0, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Raw tanh curves for different coefficients\n",
    "ax = axes[0]\n",
    "for coeff in [2, 3, 5, 10]:\n",
    "    reach_reward = 1 - np.tanh(coeff * distances)\n",
    "    ax.plot(distances, reach_reward, label=f'coeff={coeff}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Distance to cube (m)', fontsize=12)\n",
    "ax.set_ylabel('Reaching reward (before weight)', fontsize=12)\n",
    "ax.set_title('Reaching Reward Gradient\\n(lower coeff = gentler gradient)', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(x=0.3, color='yellow', linestyle='--', alpha=0.5, label='typical reach dist')\n",
    "\n",
    "# Right: Net reward (reaching - time_penalty) for different coefficients\n",
    "ax = axes[1]\n",
    "for coeff in [2, 3, 5, 10]:\n",
    "    reach_reward = (1 - np.tanh(coeff * distances)) * REACHING_WEIGHT\n",
    "    net_reward = reach_reward - TIME_PENALTY\n",
    "    ax.plot(distances, net_reward, label=f'coeff={coeff}', linewidth=2)\n",
    "\n",
    "ax.axhline(y=0, color='white', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Distance to cube (m)', fontsize=12)\n",
    "ax.set_ylabel('Net reward per step', fontsize=12)\n",
    "ax.set_title(f'Net Reward (reachÃ—{REACHING_WEIGHT} - penalty {TIME_PENALTY})\\n(closer to 0 = better signal)', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reward Heatmap: Distance vs Height (when grasping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "distances = np.linspace(0, 0.5, 50)\n",
    "heights = np.linspace(0, 0.1, 50)\n",
    "D, H = np.meshgrid(distances, heights)\n",
    "\n",
    "# Compute rewards for grasping state\n",
    "rewards_grasping = np.zeros_like(D)\n",
    "rewards_not_grasping = np.zeros_like(D)\n",
    "\n",
    "for i in range(D.shape[0]):\n",
    "    for j in range(D.shape[1]):\n",
    "        rewards_grasping[i, j] = compute_reward(D[i, j], True, H[i, j])\n",
    "        rewards_not_grasping[i, j] = compute_reward(D[i, j], False, H[i, j])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Not grasping\n",
    "ax = axes[0]\n",
    "im = ax.contourf(D, H * 100, rewards_not_grasping, levels=20, cmap='RdYlGn')\n",
    "plt.colorbar(im, ax=ax, label='Reward/step')\n",
    "ax.set_xlabel('Distance to cube (m)', fontsize=12)\n",
    "ax.set_ylabel('Height above table (cm)', fontsize=12)\n",
    "ax.set_title('Reward Landscape: NOT Grasping', fontsize=14)\n",
    "ax.axhline(y=4, color='cyan', linestyle='--', linewidth=2, label='Success height')\n",
    "ax.legend()\n",
    "\n",
    "# Right: Grasping\n",
    "ax = axes[1]\n",
    "# Mask out the success reward for clearer visualization\n",
    "rewards_grasping_masked = np.where(H >= SUCCESS_HEIGHT, np.nan, rewards_grasping)\n",
    "im = ax.contourf(D, H * 100, rewards_grasping_masked, levels=20, cmap='RdYlGn')\n",
    "plt.colorbar(im, ax=ax, label='Reward/step')\n",
    "ax.set_xlabel('Distance to cube (m)', fontsize=12)\n",
    "ax.set_ylabel('Height above table (cm)', fontsize=12)\n",
    "ax.set_title('Reward Landscape: Grasping (before success bonus)', fontsize=14)\n",
    "ax.axhline(y=4, color='cyan', linestyle='--', linewidth=2, label='Success height (+200!)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cumulative Episode Reward: Different Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 500\n",
    "steps = np.arange(max_steps)\n",
    "\n",
    "def simulate_episode(behavior, success_step=None):\n",
    "    \"\"\"Simulate cumulative reward for different behaviors.\"\"\"\n",
    "    cumulative = 0\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        if behavior == 'random_far':\n",
    "            # Far from cube, never grasps\n",
    "            r = compute_reward(dist=0.5, is_grasping=False, height_above_table=0)\n",
    "        elif behavior == 'hover_near':\n",
    "            # Near cube but never grasps\n",
    "            r = compute_reward(dist=0.05, is_grasping=False, height_above_table=0)\n",
    "        elif behavior == 'grasp_hold':\n",
    "            # Grasps but never lifts\n",
    "            r = compute_reward(dist=0.0, is_grasping=True, height_above_table=0.01)\n",
    "        elif behavior == 'success':\n",
    "            # Reaches, grasps, lifts, succeeds\n",
    "            if t < success_step * 0.3:  # Approaching\n",
    "                progress = t / (success_step * 0.3)\n",
    "                dist = 0.5 * (1 - progress)\n",
    "                r = compute_reward(dist=dist, is_grasping=False, height_above_table=0)\n",
    "            elif t < success_step * 0.6:  # Grasping\n",
    "                r = compute_reward(dist=0.0, is_grasping=True, height_above_table=0.01)\n",
    "            elif t < success_step:  # Lifting\n",
    "                lift_progress = (t - success_step * 0.6) / (success_step * 0.4)\n",
    "                height = 0.01 + lift_progress * 0.04\n",
    "                r = compute_reward(dist=0.0, is_grasping=True, height_above_table=height)\n",
    "            else:  # Success!\n",
    "                r = compute_reward(dist=0.0, is_grasping=True, height_above_table=0.05)\n",
    "                cumulative += r\n",
    "                rewards.append(cumulative)\n",
    "                break  # Episode ends on success\n",
    "        \n",
    "        cumulative += r\n",
    "        rewards.append(cumulative)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Simulate different behaviors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "behaviors = [\n",
    "    ('random_far', 'Random (far from cube)', 'red'),\n",
    "    ('hover_near', 'Hover near cube (no grasp)', 'orange'),\n",
    "    ('grasp_hold', 'Grasp and hold (no lift)', 'yellow'),\n",
    "]\n",
    "\n",
    "for behavior, label, color in behaviors:\n",
    "    rewards = simulate_episode(behavior)\n",
    "    ax.plot(range(len(rewards)), rewards, label=f'{label}: {rewards[-1]:.0f}', \n",
    "            color=color, linewidth=2)\n",
    "\n",
    "# Success at different times\n",
    "for success_step, color in [(100, 'lime'), (200, 'green'), (400, 'darkgreen')]:\n",
    "    rewards = simulate_episode('success', success_step=success_step)\n",
    "    ax.plot(range(len(rewards)), rewards, \n",
    "            label=f'Success @ step {success_step}: {rewards[-1]:.0f}',\n",
    "            color=color, linewidth=2, linestyle='--')\n",
    "\n",
    "ax.axhline(y=0, color='white', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Timestep', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Reward', fontsize=12)\n",
    "ax.set_title('Episode Reward Accumulation: Different Behaviors', fontsize=14)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_landscape(reaching_weight, reaching_coeff, grasp_reward, \n",
    "                          lift_reward, success_reward, time_penalty):\n",
    "    \"\"\"Interactive plot for tuning parameters.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Reaching gradient\n",
    "    ax = axes[0]\n",
    "    distances = np.linspace(0, 1.0, 100)\n",
    "    reach = (1 - np.tanh(reaching_coeff * distances)) * reaching_weight\n",
    "    net = reach - time_penalty\n",
    "    ax.plot(distances, net, 'cyan', linewidth=2, label='Not grasping')\n",
    "    ax.plot(distances, net + grasp_reward, 'lime', linewidth=2, label='Grasping')\n",
    "    ax.axhline(y=0, color='white', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Distance (m)')\n",
    "    ax.set_ylabel('Reward/step')\n",
    "    ax.set_title('Reward vs Distance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Lift progression\n",
    "    ax = axes[1]\n",
    "    heights = np.linspace(0, 0.1, 100)\n",
    "    lift_rewards = []\n",
    "    for h in heights:\n",
    "        r = compute_reward(0, True, h, reaching_weight, reaching_coeff,\n",
    "                          grasp_reward, lift_reward, success_reward, time_penalty)\n",
    "        lift_rewards.append(r)\n",
    "    ax.plot(heights * 100, lift_rewards, 'lime', linewidth=2)\n",
    "    ax.axvline(x=4, color='cyan', linestyle='--', label='Success threshold')\n",
    "    ax.set_xlabel('Height above table (cm)')\n",
    "    ax.set_ylabel('Reward/step')\n",
    "    ax.set_title('Reward vs Lift Height (grasping)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Episode outcomes\n",
    "    ax = axes[2]\n",
    "    outcomes = {\n",
    "        'Far\\n(500 steps)': compute_reward(0.5, False, 0, reaching_weight, reaching_coeff,\n",
    "                                           grasp_reward, lift_reward, success_reward, time_penalty) * 500,\n",
    "        'Near\\n(500 steps)': compute_reward(0.05, False, 0, reaching_weight, reaching_coeff,\n",
    "                                            grasp_reward, lift_reward, success_reward, time_penalty) * 500,\n",
    "        'Grasp+Hold\\n(500 steps)': compute_reward(0, True, 0.01, reaching_weight, reaching_coeff,\n",
    "                                                   grasp_reward, lift_reward, success_reward, time_penalty) * 500,\n",
    "        'Success\\n@ step 100': sum([compute_reward(0, True, 0.05, reaching_weight, reaching_coeff,\n",
    "                                                    grasp_reward, lift_reward, success_reward, time_penalty) \n",
    "                                    for _ in range(100)]),\n",
    "        'Success\\n@ step 300': sum([compute_reward(0, True, 0.05, reaching_weight, reaching_coeff,\n",
    "                                                    grasp_reward, lift_reward, success_reward, time_penalty) \n",
    "                                    for _ in range(300)]),\n",
    "    }\n",
    "    colors = ['red', 'orange', 'yellow', 'lime', 'green']\n",
    "    bars = ax.bar(outcomes.keys(), outcomes.values(), color=colors)\n",
    "    ax.axhline(y=0, color='white', linestyle='-', alpha=0.3)\n",
    "    ax.set_ylabel('Total Episode Reward')\n",
    "    ax.set_title('Episode Outcomes')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, outcomes.values()):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                f'{val:.0f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive sliders\n",
    "interact(plot_reward_landscape,\n",
    "         reaching_weight=FloatSlider(value=0.3, min=0.0, max=1.0, step=0.1, description='reach_wt'),\n",
    "         reaching_coeff=FloatSlider(value=3.0, min=1.0, max=10.0, step=0.5, description='reach_coeff'),\n",
    "         grasp_reward=FloatSlider(value=0.5, min=0.0, max=2.0, step=0.1, description='grasp'),\n",
    "         lift_reward=FloatSlider(value=1.0, min=0.0, max=3.0, step=0.1, description='lift'),\n",
    "         success_reward=FloatSlider(value=200.0, min=0.0, max=500.0, step=10.0, description='success'),\n",
    "         time_penalty=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.1, description='penalty'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 3D Surface: Reward vs (Distance, Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create grid\n",
    "distances = np.linspace(0, 0.5, 30)\n",
    "heights = np.linspace(0, 0.06, 30)\n",
    "D, H = np.meshgrid(distances, heights)\n",
    "\n",
    "# Compute rewards (grasping case, excluding success bonus for clarity)\n",
    "rewards = np.zeros_like(D)\n",
    "for i in range(D.shape[0]):\n",
    "    for j in range(D.shape[1]):\n",
    "        # Exclude success bonus for clearer gradient visualization\n",
    "        r = compute_reward(D[i, j], True, H[i, j], success_reward=0)\n",
    "        rewards[i, j] = r\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(D, H * 100, rewards, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('Distance to cube (m)')\n",
    "ax.set_ylabel('Height above table (cm)')\n",
    "ax.set_zlabel('Reward/step')\n",
    "ax.set_title('3D Reward Surface (Grasping, excl. success bonus)\\n'\n",
    "             'Goal: Agent should move toward origin (dist=0) and up (height=4cm+)')\n",
    "\n",
    "# Add success threshold plane\n",
    "ax.plot_surface(D, np.full_like(D, 4), np.full_like(D, rewards.max()), \n",
    "                alpha=0.2, color='cyan')\n",
    "\n",
    "plt.colorbar(surf, shrink=0.5, label='Reward/step')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Current Reward Structure\n",
    "\n",
    "With the current parameters:\n",
    "- `reaching_weight=0.3`, `reaching_coeff=3.0`\n",
    "- `grasp_reward=0.5`, `lift_reward=1.0`\n",
    "- `success_reward=200.0`, `time_penalty=0.5`\n",
    "\n",
    "**Per-step rewards:**\n",
    "| State | Reward/step |\n",
    "|-------|-------------|\n",
    "| Far from cube | ~ -0.5 |\n",
    "| Near cube (no grasp) | ~ -0.2 |\n",
    "| Grasping (not lifting) | ~ +0.3 |\n",
    "| Lifting | ~ +0.8 to +1.3 |\n",
    "| Success | +200 bonus |\n",
    "\n",
    "**Episode totals:**\n",
    "| Behavior | Total Reward |\n",
    "|----------|-------------|\n",
    "| Random far (500 steps) | ~ -250 |\n",
    "| Hover near (500 steps) | ~ -100 |\n",
    "| Grasp+hold (500 steps) | ~ +150 |\n",
    "| Success @ 100 steps | ~ +280 |\n",
    "| Success @ 300 steps | ~ +260 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
